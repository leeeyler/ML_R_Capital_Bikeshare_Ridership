---
title: "Predicting Hourly Ridership for Capital Bikeshare"
author: "Elvin Ouyang, Hellen Matarazzo, Lee Eyler, and Tarek Elduzdar"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output: html_document
abstract: "This project develops a model to forecast the bicycle rental demand in DC based on historical data, bike locations, and features of the weather. Three datasets from 2015 were utilized:  1)  Capital Bikeshare ridership, containing information for individual bike rentals throughout the year, 2)  Wunderground dataset for daily DC weather information, and 3) Google Maps API location data. Multiple Linear Regression, Decision Tree, and Random Forest models were tested.  According to our primary model performance metric, Mean Absolute Error, the Random Forest model provided the most accurate predictions"
---

```{r setup, include=FALSE}
library(plyr) # for data cleaning; one specific use-case using "revalue"
library(rpart) # Run Regression Tree model
library(rpart.plot) # Plot tree at Regression Tree model
library(randomForest) # Run Random Forest model
library(dplyr) # for data cleaning
library(lubridate) # for date cleaning
library(ggplot2) # for visualization
library(gridExtra)  # for visualization
library(reshape2) # For melt function
library(MASS) # for stepwise AIC and BIC
library(ggmap) # Extracting address information from gps info
library(stringr) # Extracting zipcodes from gps info
library(caret) # For data partition
library(plotly) # For interactive mapping and visuals
library(leaflet)
library(cowplot)
library(ggthemes)
library(viridis)
library(knitr) # to knit everything together!
library(scales)
library(hydroGOF) # For RMSE
library(rpart.plot) # Plot tree
library(rattle) # Plot tree
```
```{r}
bike_q1 <- read.csv("2015-Q1-Trips-History-Data.csv")
bike_q2 <- read.csv("2015-Q2-Trips-History-Data.csv")
bike_q3 <- read.csv("2015-Q3-cabi-trip-history-data.csv")
bike_q4 <- read.csv("2015-Q4-Trips-History-Data.csv")
weather <- read.csv('weather_2015.csv')
bike <-read.csv("Capital_Bike_Share_Locations.csv",header=TRUE, sep=",")
```

\section{1. Introduction}

In the last decade there has been increasing concern regarding the environment and the quality of life, especially in big cities. From increasing taxation to financial incentives, different approaches and public policies have been proposed and tested all around the world to address these concerns. In this scenario, shared cars and shared bicycles have became popular solutions in many cities to help mitigate traffic and environmental impact.  How can these programs be set up for success?

\section{Research Question}

Due to the increasing importance and popularity of the Capital Bikeshare program, this project aims to:  1)   identify the variables that most impact hourly ridership, and 2)  develop a model to predict hourly bikeshare demand in the Greater Washington DC region based on historical ridership and weather data for 2015.

\section{2. Methods}

\section{2.1. Dataset}

***This will be a short description of the data sets, how the data is collected, causality, bias, etc***

To conduct the analysis three datasets from 2015 were utilized: 1)  Capital Bikeshare ridership, containing information for individual bike rentals throughout the year, 2)  Wunderground dataset for daily DC weather information, and 3) Google Maps API location data.

\section{2.2.Data Wrangling, Cleaning, and Feature Engineering}

The anlaysis began by loading the R packages and the raw datasets for trip history and weather. 

With the raw datasets loaded, the first challenge was to combine the various datasets. 

The trip history datasets were addressed first.  We reviewed the variables available in each set and adjusted them as necessary to ensure consistency across the separate datasets.

```{r}
names(bike_q1)
names(bike_q2)
names(bike_q3)
names(bike_q4)
```
```{r}
bike_q3$Start.station.number <- NULL
bike_q3$End.station.number <- NULL
bike_q4$Start.station.number <- NULL
bike_q4$End.station.number <- NULL
```
```{r}
names(bike_q2)[1]<-paste("Total.duration..ms.")
names(bike_q2)[2]<-paste("Start.date")
names(bike_q2)[3]<-paste("Start.station")
names(bike_q2)[4]<-paste("End.date")
names(bike_q2)[5]<-paste("End.station")
names(bike_q2)[6]<-paste("Bike.number")
names(bike_q2)[7]<-paste("Subscription.Type")
```
```{r}
names(bike_q3)[1]<-paste("Total.duration..ms.")
names(bike_q3)[2]<-paste("Start.date")
names(bike_q3)[3]<-paste("End.date")
names(bike_q3)[4]<-paste("Start.station")
names(bike_q3)[5]<-paste("End.station")
names(bike_q3)[6]<-paste("Bike.number")
names(bike_q3)[7]<-paste("Subscription.Type")
```
```{r}
names(bike_q4)[1]<-paste("Total.duration..ms.")
names(bike_q4)[2]<-paste("Start.date")
names(bike_q4)[3]<-paste("End.date")
names(bike_q4)[4]<-paste("Start.station")
names(bike_q4)[5]<-paste("End.station")
names(bike_q4)[6]<-paste("Bike.number")
names(bike_q4)[7]<-paste("Subscription.Type")
```
```{r}
names(bike_q1)
names(bike_q2)
names(bike_q3)
names(bike_q4)
```

With variable consistency across the individual datasets, we were able to use row bind to combine Q1-Q4 of trip history data.

```{r}
bike_df <- rbind(bike_q1, bike_q2)
```
```{r}
bike_df <- rbind(bike_df, bike_q3)
```
```{r}
bike_df <- rbind(bike_df, bike_q4)
```
```{r}
dim(bike_df)
```

Next, focused on combining the full 2015 ridership dataset with the full 2015 weather dataset.  To do this, we had to identify a common variable between the datasets.  We chose to use "date" in the ymd format.  With the common variable in place in both datasets, we used a column bind to combine the ridership and weather datasets.

```{r}
bike_df$Start.date<-mdy_hm(bike_df$Start.date)
bike_df$End.date<-mdy_hm(bike_df$End.date)
str(bike_df$Start.date)
str(bike_df$End.date)
```
```{r}
bike_df$date<-date(bike_df$Start.date)
str(bike_df$date)
```
```{r}
str(weather$EST)
```
```{r}
weather$EST<-ymd(weather$EST)
str(weather$EST)
```
```{r}
names(weather)[1]<-paste("date")
str(weather$date)
```
```{r}
bike_weather <- merge(bike_df,weather,by="date")
dim(bike_weather)
names(bike_weather)
```

While the ridership data set provided some geographic information, we wanted a more robust set of geographic variables to be available, inclusive of things such as city, zip, etc.  Therefore, we used revgeocode to extract additional geographic variables from the Google Maps API using the lat/long data that was included in the ridership dataset.  The additional variables were combined to the merged ridership and weather dataset using row bind.

```{r}
# Import dataset
bike<-read.csv("Capital_Bike_Share_Locations.csv",header=TRUE, sep=",")
```
```{r}
# Extract gps info for each bike location
gps <- bike[c(3,5,6)]
```
```{r, message=FALSE, warning=FALSE}
# Extract bike full address from gps info
ad <- do.call(rbind,lapply(1:nrow(gps),function(i)revgeocode(as.numeric(gps[i,3:2])))) # Extract full address
```
```{r}
# Correct row 177
ad<-as.data.frame(ad)
levels(ad[,1])[levels(ad[,1])=="Fowler Hall, 800 Florida Ave NE, Washington, DC 20002, USA"] <- "Fowler Hall 800 Florida Ave NE, Washington, DC 20002, USA"
```
```{r}
# Transform into dataframe and separate columns
ad1<-data.frame(do.call(rbind, str_split(ad[,1], ','))) # Separate columns in variables
ad2<-data.frame(do.call(rbind, str_split(ad1[,3], ' '))) # Separate columns in variables
ad3 <- cbind(ad1[1],ad1[2],ad2[2], ad2[3], ad1[4]) # Join the variables
colnames(ad3) <- c("Address", "City", "State", "Zip", "Country") # Change variables names
gps2 <- cbind(gps,ad3) # Combine to gps the address, city, state and zip
colnames(gps2)[1] <- c("Start.station") # Change variable name to match master dataset
```
```{r}
master_df <- merge(bike_weather,gps2,by="Start.station", all.x=TRUE)
```

After the merge, we noticed there were a few observations that we needed to spot check due to missing data.  We inserted accurate information for the city variable. 

```{r}
master_df$City[master_df$Start.station=="Utah St & 11th St N "]<-"Arlington"
master_df$City[master_df$Start.station=="Veterans Pl & Pershing Dr "]<-"Silver Spring"
master_df$City[master_df$Start.station=="Washington Blvd & Walter Reed Dr "]<-"Arlington"
master_df$City[master_df$Start.station=="8th & F St NW"]<-"Washington"
master_df$City[master_df$Start.station=="Lee Hwy & N Nelson St"]<-"Arlington"
master_df$City[master_df$Start.station=="11th & K St NW"]<-"Washington"
master_df$City[master_df$Start.station=="20th & Bell St"]<-"Arlington"
master_df$City[master_df$Start.station=="23rd & E St NW "]<-"Washington"
master_df$City[master_df$Start.station=="34th & Water St NW"]<-"Washington"
master_df$City[master_df$Start.station=="34th St & Minnesota Ave SE"]<-"Washington"
master_df$City[master_df$Start.station=="Anacostia Ave & Benning Rd NE / River Terrace "]<-"Washington"
master_df$City[master_df$Start.station=="Court House Metro / 15th & N Uhle St "]<-"Arlington"
master_df$City[master_df$Start.station=="Fenton St & Ellsworth Dr "]<-"Silver Spring"
master_df$City[master_df$Start.station=="King St Metro"]<-"Alexandria"
master_df$City[master_df$Start.station=="Lincoln Park / 13th & East Capitol St NE "]<-"Washington"
master_df$City[master_df$Start.station=="Montgomery Ave & Waverly St "]<-"Bethesda"
master_df$City[master_df$Start.station=="N Adams St & Lee Hwy"]<-"Arlington"
master_df$City[master_df$Start.station=="N Quincy St & Wilson Blvd"]<-"Arlington"
master_df$City[master_df$Start.station=="S Abingdon St & 36th St S"]<-"Arlington"
master_df$City[master_df$Start.station=="N Nelson St & Lee Hwy"]<-"Arlington"
master_df$City[master_df$Start.station=="Fenton St & New York Ave "]<-"Silver Spring"
master_df$City[master_df$Start.station=="Alta Tech Office"]<-"Washington"
```

\section{Data Cleaning and Feature Engineering}

With all of the individual data sets combined into a singular master dataframe, we moved onto the data cleaning stage.  

The primary purpose of the data cleaning stage was to make variable data type transformations  and address any missing values so that the data would be easier to work with in the exploratory data analysis (EDA) and modeling stages.  

The primary purpose of the feature engineering stage was to create new variables based on combinations or derivatives of existing variables.  Our inutition was that these variables would make a large impact in the prediction models.

The following sequence of commentary and code describes the data cleaning and feature engineering that was conducted.

Changed format of date variables using lubridate package.

```{r}
master_df$date<- ymd(master_df$date)
master_df$Start.date<- ymd_hms(master_df$Start.date)
master_df$End.date<- ymd_hms(master_df$End.date)
head(master_df$date)
head(master_df$Start.date)
head(master_df$End.date)
```

Changed precipitation variable to type numeric.

```{r}
master_df$PrecipitationIn<- as.numeric(as.character(master_df$PrecipitationIn))
```

Created a new hour variable.

```{r}
master_df$hour<- hour(master_df$Start.date)
```

Added a binary weekday variable.

```{r}
master_df$weekday<- weekdays(master_df$date)
```

Created a binary weekend variable.

```{r}
master_df$weekend<- ifelse(master_df$weekday=="Saturday"|master_df$weekday=="Sunday",1,0)
```

Created a binary rush hour variable.

```{r}
master_df$rushhour<- ifelse(master_df$hour<=9 & master_df$hour>=7 | master_df$hour<=19 & master_df$hour>=16,master_df$rushhour<-1,master_df$rushhour<-0)
```

Created a binary holiday variable.

```{r}
master_df$holiday <- ifelse(master_df$date=='2015-01-01' | master_df$date=='2015-01-19' | master_df$date=='2015-02-16'|master_df$date=='2015-04-16'| master_df$date=='2015-04-17'| master_df$date=='2015-05-22'|master_df$date=='2015-05-25'| master_df$date=='2015-05-26'| master_df$date=='2015-07-02' | master_df$date=='2015-07-03' | master_df$date=='2015-07-06' | master_df$date=='2015-09-04' | master_df$date=='2015-09-07' | master_df$date=='2015-09-08' | master_df$date=='2015-10-12' | master_df$date=='2015-11-11' | master_df$date=='2015-11-26' | master_df$date=='2015-11-27' | master_df$date=='2015-12-24' | master_df$date=='2015-12-25' | master_df$date=='2015-12-31', master_df$holiday<-1,master_df$holiday<-0)
```

Combined the holiday and weekend variables for a new binary variable.

```{r}
master_df$weekend_holiday<- ifelse(master_df$weekend==1 | master_df$holiday == 1,1,0)
```

Revalued subscription type, as certain datasets used "Member" and other datasets used "Registered" to signify a "subscriber".  Therefore, we combined these values.

```{r}
master_df$Subscription.Type<-revalue(master_df$Subscription.Type,c('Member'='Registered'))
detach(package:plyr)
```

Created feels like temperature variable.

```{r}
master_df$Mean.Humidity<-master_df$Mean.Humidity/100
master_df$feellike<-0.363445176+
  0.98862246*(master_df$Mean.TemperatureF)+
  4.777114035*(master_df$Mean.Humidity)+
  -0.114037667*(master_df$Mean.TemperatureF*master_df$Mean.Humidity)+
  -0.000850208*(master_df$Mean.TemperatureF^2)+
  -0.020716198*(master_df$Mean.Humidity^2)+
  0.000687678*((master_df$Mean.TemperatureF^2)*master_df$Mean.Humidity)+
  0.000274954*(master_df$Mean.TemperatureF*(master_df$Mean.Humidity^2))+
  0*((master_df$Mean.TemperatureF^2)*(master_df$Mean.Humidity^2))
```

Created season variable.

```{r}
getSeason <- function(DATES) {
  WS <- as.Date("2012-12-15", format="%Y-%m-%d") # Winter Solstice
  SE <- as.Date("2012-03-15", format="%Y-%m-%d") # Spring Equinox
  SS <- as.Date("2012-06-15", format="%Y-%m-%d") # Summer Solstice
  FE <- as.Date("2012-09-15", format="%Y-%m-%d") # Fall Equinox
  d <- as.Date(strftime(DATES, format="2012-%m-%d"))
  ifelse (d>=WS|d<SE, "Winter", ifelse (d>=SE&d<SS,"Spring", ifelse(d>=SS&d<FE,"Summer","Fall")))
}
master_df$season<-getSeason(master_df$date)
master_df$season<-as.factor(master_df$season)
summary(master_df$season)
```

Created a variable for adverse weather.

```{r}
master_df$AdverseWeather <- ifelse(master_df$Events == "","False","True")
master_df$AdverseWeather <- as.factor(master_df$AdverseWeather)
summary(master_df$AdverseWeather)
```

Create a variable beautiful weather.

```{r}
master_df$BeautifulWeather <- ifelse(master_df$Events == "" & master_df$Mean.TemperatureF >= 50 & master_df$Mean.TemperatureF <= 85,"True","False")
master_df$BeautifulWeather <- as.factor(master_df$BeautifulWeather)
summary(master_df$BeautifulWeather)
```

Remove the whitespace from city variable.

```{r}
levels(master_df$City)
levels(master_df$City)[levels(master_df$City)==" Washington"]<-"Washington"
levels(master_df$City)[levels(master_df$City)==" Alexandria"]<-"Alexandria"
levels(master_df$City)[levels(master_df$City)==" Arlington"]<-"Arlington"
levels(master_df$City)[levels(master_df$City)==" Bethesda"]<-"Bethesda"
levels(master_df$City)[levels(master_df$City)==" Chevy Chase"]<-"Chevy Chase"
levels(master_df$City)[levels(master_df$City)==" Derwood"]<-"Derwood"
levels(master_df$City)[levels(master_df$City)==" McLean"]<-"McLean"
levels(master_df$City)[levels(master_df$City)==" Potomac"]<-"Potomac"
levels(master_df$City)[levels(master_df$City)==" Reston"]<-"Reston"
levels(master_df$City)[levels(master_df$City)==" Rockville"]<-"Rockville"
levels(master_df$City)[levels(master_df$City)==" Silver Spring"]<-"Silver Spring"
levels(master_df$City)[levels(master_df$City)==" Takoma Park"]<-"Takoma Park"
levels(master_df$City)[levels(master_df$City)==" Tysons"]<-"Tysons"
levels(master_df$City)[levels(master_df$City)==" Vienna"]<-"Vienna"
levels(master_df$City)
```

Created a month variable.

```{r}
master_df$month<- month(master_df$date)
```

The precipitation variable was missing quite a few values.  However, we knew that we could fill these missing values in with our best guess by using the event variable and the average precipitation by month.  

If the weather event variable was blank, we knew it was a good weather day.  On the contrary, if the weather event variable was not blank, we knew it was likely that rain was experienced.  We therefore, filled in the missing value with the average precipitation for the respective month.

```{r}
average_month_precipitation<-master_df%>%
  select(month,PrecipitationIn)%>%
  group_by(month)%>%
  summarise(mean(PrecipitationIn,na.rm=TRUE))
```
```{r}
colnames(average_month_precipitation)[2]<-"avg_precipitation"
```
```{r}
master_df <- merge(master_df,average_month_precipitation,by="month")
```
```{r}
master_df$new_precipitation<-
  ifelse(is.na(master_df$PrecipitationIn) & !master_df$Events=="",
         master_df$new_precipitation<- master_df$avg_precipitation,
         ifelse(is.na(master_df$PrecipitationIn) & master_df$Events=="",master_df$new_precipitation<-0.000000,
                master_df$new_precipitation<- master_df$PrecipitationIn))
```
```{r}
#Drop variable used for calculation
master_df$avg_precipitation<-NULL
```

Factorized weekday, weekend, holiday, rushhour, cloud cover, hour, and zip variables.

```{r}
master_df$weekday<- as.factor(master_df$weekday)
master_df$weekday<- factor(master_df$weekday, levels = c("Monday", "Tuesday", "Wednesday","Thursday","Friday","Saturday","Sunday"))
master_df$weekend<- as.factor(master_df$weekend)
master_df$holiday<- as.factor(master_df$holiday)
master_df$rushhour<- as.factor(master_df$rushhour)
master_df$weekend_holiday<- as.factor(master_df$weekend_holiday)
master_df$CloudCover<- as.factor(master_df$CloudCover)
master_df$hour<- as.factor(master_df$hour)
master_df$Zip<- as.factor(master_df$Zip)
```

The final list of variables and the dimensions of the dataframe.

```{r}
dim(master_df)
names(master_df)
```

Saved a copy of the master dataset.

```{r}
write.csv(master_df,"master_df.csv")
```

\section{2.3 Exploratory Data Analysis}

The data exploration stage focused on visualizing the relationships between variables and exploring  patterns within the dataset. 

The following sequence of commentary and code showcases the EDA that was conducted.

```{r}
# Fine tune master_df before creating EDAs
master_df %>%
  mutate(
    date = ymd(date),
    weekday = factor(weekday,
                     levels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")),         season = factor(season, levels = c("Spring", "Summer", "Fall", "Winter")),
    hour = factor(hour, levels = 0:23),
    duration.min = round(Total.duration..ms. / 60000, digits = 2)
  ) %>%
  dplyr::select(Start.station, date, duration.min, End.station, Subscription.Type, CloudCover, Events, LATITUDE, LONGITUDE, Address, City, Zip, hour, weekday, weekend, rushhour, holiday, season, AdverseWeather, BeautifulWeather, weekend_holiday) -> map_df
```

The first thing we'll do is to run distribution analysis on the main continuous variables in the dataset: total.rides and avg.duration. We will use levels of five categorical variables, i.e. Subscription.Type, weekend_holiday, rushhour, season, and AdverseWeather, as group coloring to generate high level between-group distribution comparison.

```{r, message=FALSE, warning=FALSE}
map_df %>%
  group_by(date, hour, Subscription.Type) %>%
  summarise(
    total.rides = n(),
    avg.duration = mean(duration.min), 
    weekend_holiday = first(weekend_holiday),
    weekday = first(weekday),
    rushhour = first(rushhour),
    season = first(season),
    AdverseWeather = first(AdverseWeather)
  ) -> day_hour_rides
```
```{r}
# Create distribution histograms
g1 <- ggplot(data=day_hour_rides)
g1 + geom_histogram(mapping = aes(total.rides, fill = Subscription.Type), binwidth = 0.5) -> g2
g1 + geom_histogram(mapping = aes(log(day_hour_rides$avg.duration), fill = Subscription.Type), bins = 100) -> g3
g1 + geom_histogram(mapping = aes(total.rides, fill = weekend_holiday), binwidth = 0.5) -> g4
g1 + geom_histogram(mapping = aes(log(day_hour_rides$avg.duration), fill = weekend_holiday), bins = 100) -> g5
g1 + geom_histogram(mapping = aes(total.rides, fill = as.factor(rushhour)), binwidth = 0.5) -> g6
g1 + geom_histogram(mapping = aes(log(day_hour_rides$avg.duration), fill = as.factor(rushhour)), bins = 100) -> g7
g1 + geom_histogram(mapping = aes(total.rides, fill = as.factor(season)), binwidth = 0.5) -> g8
g1 + geom_histogram(mapping = aes(log(day_hour_rides$avg.duration), fill = as.factor(season)), bins = 100) -> g9
g1 + geom_histogram(mapping = aes(total.rides, fill = as.factor(AdverseWeather)), binwidth = 0.5) -> g10
g1 + geom_histogram(mapping = aes(log(day_hour_rides$avg.duration),
                                  fill = as.factor(AdverseWeather)), bins = 100) -> g11
```
```{r}
# Display the histograms
plot_grid(g2, g3, nrow = 2, rel_widths = c(1/2, 1/2))
plot_grid(g4, g5, nrow = 2, rel_widths = c(1/2, 1/2))
plot_grid(g6, g7, nrow = 2, rel_widths = c(1/2, 1/2))
plot_grid(g8, g9, nrow = 2, rel_widths = c(1/2, 1/2))
plot_grid(g10, g11, nrow = 2, rel_widths = c(1/2, 1/2))
```

Our first impression is that the distribution of total.rides is skewing right, while the distribution of avg.duration has two modes.

More specifically, the avg.duration distribution by Subscription.Type graph indicates that registered bikers are contributing to the lower duration mode while the casual bikers are contrbution to the higher mode. Casual bikers have much less total.rides than the registered bikers. In the distribution by rushhour graph, commuting hour rides are dominating hours that have higher count of total.rides. Rushhour rides are also contributing more to the lower avg.duration mode. Another interesting finding from the distribution by season graph is that winter has much more short-duration rides than other seasons, while spring and summer have more long-duration rides among casual riders.

The above analysis indicates that time-related factors are having a strong impact on the dependent variables. In our next step, we will create heatmaps for hour of the day / day of the week to futher explore the patterns.

```{r, message=FALSE, warning=FALSE}
# Create a subset just for the time heatmap
day_hour_rides %>%
  ungroup() %>%
  select(hour, weekday, total.rides, avg.duration) %>%
  mutate(total_duration = total.rides * avg.duration, 
         hour = factor(hour, levels = (0:23))) %>%
  group_by(hour, weekday) %>%
  summarise(count.rides = sum(total.rides), total.duration = sum(total_duration)) -> df.1
```
```{r, message=FALSE, warning=FALSE}
# Create time based heatmaps
g10 <- ggplot(data=df.1, aes(x=hour, y=weekday, fill=count.rides)) +
  geom_tile(color="white", size=0.1)+ coord_equal() +
  labs(x=NULL, y=NULL, title="Count of Rides Per Weekday & Hour of Day") +
  theme_tufte(base_family="Calibri") + theme(plot.title=element_text(hjust=0.5, size = 10)) +
  theme(axis.ticks=element_blank()) + theme(axis.text=element_text(size=7)) + theme(legend.position="none") +
  scale_fill_gradient(low = "white", high = "steelblue")
g11 <- ggplot(data=df.1, aes(x=hour, y=weekday, fill=total.duration)) +
  geom_tile(color="white", size=0.1)+ coord_equal() +
  labs(x=NULL, y=NULL, title="Total Duration Per Weekday & Hour of Day") +
  theme_tufte(base_family="Calibri") + theme(plot.title=element_text(hjust=0.5, size = 10)) + theme(legend.position="none") +
  theme(axis.ticks=element_blank()) + theme(axis.text=element_text(size=7)) +
  scale_fill_gradient(low = "white", high = "firebrick")
g12 <- ggplot(data=df.1, aes(x=hour, y=weekday, fill=total.duration/count.rides)) +
  geom_tile(color="white", size=0.1)+ coord_equal() +
  labs(x=NULL, y=NULL, title="Average Duration Per Weekday & Hour of Day") +
  theme_tufte(base_family="Calibri") + theme(plot.title=element_text(hjust=0.5, size = 10)) + theme(legend.position="none") +
  theme(axis.ticks=element_blank()) + theme(axis.text=element_text(size=7)) +
  scale_fill_gradient(low = "white", high = "springgreen3")
plot_grid(g10, g12, nrow = 2, rel_heights = c(1/2, 1/2))
```

Here we find some interesting patterns from the hour-weekday heatmap. It seems that more rides have taken place during rush hours on work days, while total.rides distributes evenly in day time on weekend. The avg.duration of the rides appears to be longer during day time over the weekend.

After we have a general understanding of the data, we move on to explore the geospatial distribution of total.rides across the DC metro area. First let us plot the bike stations.

```{r}
# Create station list with coordinates, total count of rides, and total duration of rides
map.stations <- map_df %>%
  group_by(Start.station) %>%
  summarise(total.rides = n(),
            avg.duration = mean(duration.min),
            subscriber.percentage = mean(Subscription.Type == "Registered"),
            lat = first(LATITUDE),
            lon = first(LONGITUDE)
            )
head(map.stations)
```
```{r}
# Plotly not working, skip
g <- list(
  scope = 'usa',
  projection = list(type = 'albers usa'),
  showland = TRUE,
  landcolor = toRGB("gray85"),
  subunitwidth = 1,
  countrywidth = 1,
  subunitcolor = toRGB("white"),
  countrycolor = toRGB("white")
)

p <- plot_geo(map.stations, locationmode = 'city', sizes = c(1, 250)) %>%
  add_markers(
    x = ~lon, y = ~lat, size = ~total.rides, color = ~avg.duration, hoverinfo = "text",
    text = ~paste(map.stations$Start.station, "<br />",
                  "Total Rides: ", map.stations$total.rides, "<br />", 
                  "Average Duration: ", map.stations$avg.duration, " mins",
                  "Percentage of Subscribers: ", map.stations$subscriber.percentage)
  ) %>%
  layout(title = '2015 Capital Bike Share Stations', geo = g)
```

Below we can see the locations of all the bike share stations across the DMV area, with the circle size representing total.rides and color representing avg.rides. It appears that bike stations are spreading out well in the DMV area, with stations located in DMV ourskirts such as Alexandria, VA, Bethesda, MD, and Silver Spring, MD.

```{r, message=FALSE, warning=FALSE}
# download basic map layers for plotting
base.map <- qmap("Wasington DC", zoom = 12, source= "google", maptype="roadmap", color = "bw", crop=FALSE, legend='topleft')
base.map.1 <- qmap("Wasington DC", zoom = 13, source= "google", maptype="roadmap", color = "bw", crop=FALSE, legend='topleft')
base.map.2 <- qmap("Wasington DC", zoom = 14, source= "google", maptype="roadmap", color = "bw", crop=FALSE, legend='topleft')
```

```{r, message=FALSE, warning=FALSE}
base.map + geom_point(aes(x = lon, y = lat, size=total.rides, color=avg.duration), data = map.stations,
 alpha = .5)+ scale_size(range = c(1, 5)) + scale_colour_gradient(low = "steelblue", high = "springgreen")
base.map.1 + geom_point(aes(x = lon, y = lat, size=total.rides, color=avg.duration), data = map.stations,
 alpha = .5) + scale_size(range = c(1, 5)) + scale_colour_gradient(low = "steelblue", high = "springgreen")
base.map.2 + geom_point(aes(x = lon, y = lat, size=total.rides, color=avg.duration), data = map.stations,
 alpha = .5) + scale_size(range = c(1, 10)) + scale_colour_gradient(low = "steelblue", high = "springgreen")

```

But how does the actual count of total.rides distribute across the area? Will it go in line with the bike station locations? We then move on to create a heatmap based on the density of total.rides on the map. The graph below indicates that total.rides are way more condensed than the distribution of the bike stations, with the most rides happening in the DC heart area, such as Dupont Circle, Logan Circle, National Mall, Metro Center, Gallery Place, World Bank, and Lincoln Memorial.

```{r, message=FALSE, warning=FALSE}
# Create a ride data set with location and ride, will also keep sliceability with other factors
# Adjust factor level names for better display in faceted visuals
map_df %>%
  mutate(lon = LONGITUDE, lat = LATITUDE) %>%
  select(Subscription.Type, Events, lat, lon, hour,
         weekday, weekend, rushhour, holiday, season, AdverseWeather, BeautifulWeather, weekend_holiday) %>%
  mutate(
    hour = as.numeric(hour),
    AdverseWeather = as.factor(if_else(AdverseWeather=="True", "Adverse: Yes", "Adverse: No")),
    BeautifulWeather = as.factor(if_else(BeautifulWeather == "True", "Beautiful: Yes", "Beautiful: No")),
    holiday = as.factor(if_else(holiday == "1", "Holiday: Yes", "Holiday: No")),
    weekend = as.factor(if_else(weekend == "1", "Weekend: Yes", "Weekend: No")),
    rushhour = as.factor(if_else(rushhour == "1", "Rush Hour: Yes", "Rush Hour: No")),
    weekend_holiday = as.factor(if_else(weekend_holiday == "1", "Leisure Day: Yes", "Leisure Day: No")),
    time_of_day = factor(if_else(hour>4 & hour < 13, "Morning",
                                    if_else(hour>12 & hour < 19, "Afternoon", 
                                            if_else(hour >16 & hour <= 23, "Night", "Late Night"))),
                            levels = c("Morning", "Afternoon", "Night", "Late Night")),
    hour = factor(hour, levels = 0:23)) -> ride_df
```
```{r, message=FALSE, warning=FALSE}
# Create ride density maps
base.map + geom_density2d(data = ride_df[sample(1:nrow(ride_df), 10000),], 
    aes(x = lon, y = lat), size = 0.4) + stat_density2d(data = ride_df[sample(1:nrow(ride_df), 10000),], 
    aes(x = lon, y = lat, fill = ..level.., alpha = ..level..), size = 1, 
    bins = 5, geom = "polygon", contour = TRUE) + scale_fill_gradient(low = "springgreen", high = "red") + 
    scale_alpha(range = c(0, 0.3), guide = FALSE)

base.map.1 + geom_density2d(data = ride_df[sample(1:nrow(ride_df), 10000),], 
    aes(x = lon, y = lat), size = 0.4) + stat_density2d(data = ride_df[sample(1:nrow(ride_df), 10000),], 
    aes(x = lon, y = lat, fill = ..level.., alpha = ..level..), size = 2, 
    bins = 8, geom = "polygon", contour = TRUE) + scale_fill_gradient(low = "springgreen", high = "red") + 
    scale_alpha(range = c(0, 0.3), guide = FALSE)

base.map.2 + geom_density2d(data = ride_df[sample(1:nrow(ride_df), 10000),], 
    aes(x = lon, y = lat), size = 0.5) + stat_density2d(data = ride_df[sample(1:nrow(ride_df), 10000),], 
    aes(x = lon, y = lat, fill = ..level.., alpha = ..level..), size = 3, 
    bins = 15, geom = "polygon", contour = TRUE) + scale_fill_gradient(low = "springgreen", high = "red") + 
    scale_alpha(range = c(0, 0.3), guide = FALSE)
```
Since we now have a general idea of where the most rides are happening in DC, our next step is to slice the ridership data with factors we generated from time and weather and compare the patterns. We wanted to see if the popularity of the stations changed under different time and weather conditions.

```{r}
# Create a subsliced ridership set of 15000 observations
ride_df.sample <- ride_df[sample(1:nrow(ride_df), 15000),]
```
```{r, message=FALSE, warning=FALSE, include=FALSE}
# Create base layers for faceted mapping
dc.1 <- get_map('washington dc', zoom = 12, source = "google", maptype = "roadmap", crop=FALSE, color="bw")
dc.2 <- get_map('washington dc', zoom = 13, source = "google", maptype = "roadmap", crop=FALSE, color="bw")
dc.3 <- get_map('washington dc', zoom = 14, source = "google", maptype = "roadmap", crop=FALSE, color="bw")
dc.map.1 <- ggmap(dc.1, base_layer = ggplot(aes(x = lon, y = lat), data = ride_df.sample))
dc.map.2 <- ggmap(dc.2, base_layer = ggplot(aes(x = lon, y = lat), data = ride_df.sample))
dc.map.3 <- ggmap(dc.3, base_layer = ggplot(aes(x = lon, y = lat), data = ride_df.sample))
```
```{r, message=FALSE, warning=FALSE}
# Ride frequency heatmap by seasons
dc.map.3 + stat_density2d(aes(x=lon, y=lat, fill=..level.., alpha=..level..),
                          bins=7, geom="polygon", data=ride_df.sample) +
  scale_fill_gradient(low="springgreen", high="tomato") + scale_alpha(range = c(0.1, 0.6), guide = FALSE) + 
  facet_wrap(~season, nrow = 1) +
  guides(fill=guide_legend(title="ride\nfrequency")) +
  ggtitle("Ride Distribution by Seasons") +
  theme(axis.title=element_blank(),
        axis.text=element_blank(),
        axis.ticks=element_blank(),
        legend.text = element_blank(),
        plot.title = element_text(color="black", size=16, hjust=0)) 
```

The first graph shows the distribution of rides in each season of the year of 2015. In Spring and Summer, both Lincoln Memorial and National Mall enjoy more rides from other time of the year. During winter, however, it seems that more people are taking bike rides around Logan Circle, Foggy Bottom, and Metro Center, i.e. the inner center of the District.

```{r, message=FALSE, warning=FALSE}
# Ride frequency heatmap by time of day
dc.map.3 + stat_density2d(aes(x=lon, y=lat, fill=..level.., alpha=..level..),
                          bins=7, geom="polygon", data=ride_df.sample) +
  scale_fill_gradient(low="springgreen", high="tomato") + scale_alpha(range = c(0.1, 0.6), guide = FALSE) + 
  facet_wrap(~time_of_day, nrow = 1) +
  guides(fill=guide_legend(title="ride\nfrequency")) +
  ggtitle("Ride Distribution by Time of Day") +
  theme(axis.title=element_blank(),
        axis.text=element_blank(),
        axis.ticks=element_blank(),
        legend.text = element_blank(),
        plot.title = element_text(color="black", size=16, hjust=0)) 
```

Another similar comparison based on time of the day shows that people are taking more rides in central to northeastern DC in the morning and more in central to southwestern DC in the afternoon. Bikers start their rides mostly around DuPont circle, Logan Circle, Metro Center, and Gallery Place at night. Few people will start their rides in late night, of course; but we are seeing relatively more rides in the central to northwestern DC area. It seems that people's daily routine is contributing to this pattern, considering that these areas correspond to the residence area, working area, and entertaining/event area in DC.

```{r, message=FALSE, warning=FALSE}
# Ride frequency heatmap by rush hour
dc.map.3 + stat_density2d(aes(x=lon, y=lat, fill=..level.., alpha=..level..),
                          bins=7, geom="polygon", data=ride_df.sample) +
  scale_fill_gradient(low="springgreen", high="tomato") + scale_alpha(range = c(0.1, 0.6), guide = FALSE) + 
  facet_wrap(~rushhour) +
  guides(fill=guide_legend(title="ride\nfrequency")) +
  ggtitle("Ride Distribution - Rush Hour?") +
  theme(axis.title=element_blank(),
        axis.text=element_blank(),
        axis.ticks=element_blank(),
        legend.text = element_blank(),
        plot.title = element_text(color="black", size=16, hjust=0))
```

Since time is creating interesting impact on total.rides and bikes can be a useful tool for commuting, we want to check out specifically the allocation of rides for rush hours againt other time of the day. In the above graph, we notice that more people are taking bike rides near Metro Center, Gallery Place, and Capital Hill during rush hours, while more people are taking rides near Lincoln Memorial and National Mall during non-rush hours. This information is interesting, since Metro center, Gallery place, and Capital Hill are places where many people go to work, while (apparently) Lincoln Memorial and National Mall are popular tourist sites.

```{r, message=FALSE, warning=FALSE}
# Ride frequency heatmap by weekend/holiday
dc.map.3 + stat_density2d(aes(x=lon, y=lat, fill=..level.., alpha=..level..),
                          bins=7, geom="polygon", data=ride_df.sample) +
  scale_fill_gradient(low="springgreen", high="tomato") + scale_alpha(range = c(0.1, 0.6), guide = FALSE) + 
  facet_wrap(~weekend_holiday + BeautifulWeather, nrow = 1) +
  guides(fill=guide_legend(title="ride\nfrequency")) +
  ggtitle("Ride Distribution - Leisure Days X Good Weather") +
  theme(axis.title=element_blank(),
        axis.text=element_blank(),
        axis.ticks=element_blank(),
        legend.text = element_blank(),
        plot.title = element_text(color="black", size=16, hjust=0))
```

Since Lincoln Memorial and National Mall are enjoying much love in non-rush hours, we are interested to check out if leisure time will have a different pattern for total.rides distribution. Comparing the left two graphs in the above chart, it is apparent that the distribution of ridership is sparse for leisure days in good weather: riders are of course starting their rides from many different stations across the District. Interestingly, the second left graph shows that bikers mostly still ride in the central DC during working days despite the good weather. Commuting really seems to be a major function of the shared bikes!

Since commuting seems to be a really big factor for the distribution of rides, we are insterested to dig a bit deeper into the type of subscription for each ride. Since bike share subscribers are more likely to use bikes for commute, will we see a clear difference between casual and registered bikers?

```{r, message=FALSE, warning=FALSE}
# Ride frequency heatmap by Subscription Type
dc.map.2 + stat_density2d(aes(x=lon, y=lat, fill=..level.., alpha=..level..),
                          bins=7, geom="polygon", data=ride_df.sample) +
  scale_fill_gradient(low="springgreen", high="tomato") + scale_alpha(range = c(0.1, 0.6), guide = FALSE) + 
  facet_wrap(~Subscription.Type + rushhour, nrow = 1) +
  guides(fill=guide_legend(title="ride\nfrequency")) +
  ggtitle("Ride Distribution by Subscription Type & Rush Hour") +
  theme(axis.title=element_blank(),
        axis.text=element_blank(),
        axis.ticks=element_blank(),
        legend.text = element_blank(),
        plot.title = element_text(color="black", size=16, hjust=0))
```

The above graph shows that casual bikers are (apparently) taking more rides around the tourist attraction sites in DC, no matter if it's in rush hour or not. For the subscribers, however, the distribution of rides are surprisingly even no matter it's rush hour or not. If we really consider the nature of commuting, this actually makes sense: for people that ride bikes based on their daily commuting needs, they will need to use bikes to get to work or go home. The green area in the right two graphs actually shows the routine start stations for the registered users!

```{r, message=FALSE, warning=FALSE}
# Ride frequency heatmap by adverse weather
dc.map.2 + stat_density2d(aes(x=lon, y=lat, fill=..level.., alpha=..level..),
                          bins=7, geom="polygon", data=ride_df.sample) +
  scale_fill_gradient(low="springgreen", high="tomato") + scale_alpha(range = c(0.1, 0.6), guide = FALSE) + 
  facet_wrap(~AdverseWeather) +
  guides(fill=guide_legend(title="ride\nfrequency")) +
  ggtitle("Ride Distribution - Adverse Weather?") +
  theme(axis.title=element_blank(),
        axis.text=element_blank(),
        axis.ticks=element_blank(),
        legend.text = element_blank(),
        plot.title = element_text(color="black", size=16, hjust=0))
```

A quick comparison of adverse weather against non-adverse weather shows not much difference for the ridership. This might be due to the nature of our integrated weather data: the weather information is the mean values for a whole day, thus making it hard for the slicers to differentiate ridership distribution on a lower grain level. 

```{r, message=FALSE, warning=FALSE}
# Ride frequency heatmap by rush hour and adverse weather
dc.map.3 + stat_density2d(aes(x=lon, y=lat, fill=..level.., alpha=..level..),
                          bins=7, geom="polygon", data=ride_df.sample) +
  scale_fill_gradient(low="springgreen", high="tomato") + scale_alpha(range = c(0.1, 0.6), guide = FALSE) + 
  facet_wrap(~AdverseWeather + rushhour, nrow = 1) +
  guides(fill=guide_legend(title="ride\nfrequency")) +
  ggtitle("Ride Distribution - Bad Weather X Rush Hour") +
  theme(axis.title=element_blank(),
        axis.text=element_blank(),
        axis.ticks=element_blank(),
        legend.text = element_blank(),
        plot.title = element_text(color="black", size=16, hjust=0))
```

Again, in the graph shown above here, we observe a bigger differece from Rush Hour than the weather. This seems to be related to the same challenge we are having from the weather variables. 

\section{3. Modeling}

In total, three different models were tested: Multiple Linear Regression, Regression Tree and Random Forest. The prediction performance of the models was assessed based on the Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE).

Before creating the models, the final dataset was grouped by day and by hour since this was the level of ridership that we wanted to predicted.  We also dropped unnecessary variables that would not be used for modeling.  Lastly, we split the final data set into test and train dataframes based on a 70/30 random sampling split.

```{r}
model_df <- master_df %>% 
        group_by(date,hour,Subscription.Type,Mean.TemperatureF,MeanDew.PointF,Mean.Humidity,Mean.Sea.Level.PressureIn,Mean.VisibilityMiles,Mean.Wind.SpeedMPH,new_precipitation,CloudCover,Events,City,weekday,weekend,rushhour,weekend_holiday,feellike,season,AdverseWeather,BeautifulWeather) %>% 
        summarise(total_rides = length(date))

dim(model_df)
names(model_df)
```
```{r}
smp_size <- floor(0.7 * nrow(model_df))
set.seed(700)
train_ind <- sample(seq_len(nrow(model_df)), size = smp_size)
linearreg_train <- model_df[train_ind, ]
linearreg_test <- model_df[-train_ind, ]
```

\section{Model 1 - Multiple Linear Regression Model}

A collinearity test was conducted for the numeric variables.

```{r}
num_vars <- c("Mean.TemperatureF","MeanDew.PointF","Mean.Humidity","Mean.Sea.Level.PressureIn","Mean.VisibilityMiles","Mean.Wind.SpeedMPH", "new_precipitation")

collinear_test_df <- linearreg_train[num_vars]

plot(collinear_test_df)
```
```{r}
qplot(x=Var1, y=Var2, data = melt(cor(collinear_test_df)), fill=value, geom = "tile") + 
        labs(xlab = "Var1", ylab = "Var2") + 
        ggtitle("Correlation Coefficient Matrix")
```

Mean.TemperatureF and MeanDew.PointF showed a high correlation, so MeanDew.PointF was dropped.  Date was also dropped, as including the variable would have lead to overfitting and also created a factor with far too many levels to be included in the model.

With the list of variables finalized, three modeling selection techniques were tested:  Adjusted R Squared, AIC, and BIC.  These techniques use different methods for penalizing the inclusion of each additional variable within the model, so we were interested to understand the impact this would have on each models prediction.

```{r}
linearreg_train$MeanDew.PointF <- NULL
linearreg_train$date <- NULL
names(linearreg_train)
```
```{r}
m_full_linear <- lm(total_rides ~ ., data = na.omit(linearreg_train))
summary(m_full_linear)
str(master_df)
```
```{r}
anova(m_full_linear)
```
```{r}
n = nrow(na.omit(linearreg_train))
stepAIC(na.omit(m_full_linear), k=log(n)) #BIC
```
```{r}
stepAIC(na.omit(m_full_linear), k=2 )#AIC
```

Results of variable selection for each technique:

Adjusted R Squared:  hour + Subscription.Type + Mean.TemperatureF + Mean.Humidity + Mean.Sea.Level.PressureIn + Mean.VisibilityMiles + Mean.Wind.SpeedMPH + new_precipitation + CloudCover + City + weekday + rushhour + weekend_holiday + feellike + season + BeautifulWeather

BIC:  hour + Subscription.Type + Mean.TemperatureF + Mean.Humidity + Mean.VisibilityMiles + Mean.Wind.SpeedMPH + new_precipitation + City + feellike + season + BeautifulWeather

AIC:  hour + Subscription.Type + Mean.TemperatureF + Mean.Humidity + Mean.VisibilityMiles + Mean.Wind.SpeedMPH + new_precipitation + CloudCover + City + weekday + weekend_holiday +feellike + season + BeautifulWeather

Based on these results, models were created for each technique.

```{r}
m_full_linear <- lm(total_rides ~ hour + Subscription.Type + Mean.TemperatureF + Mean.Humidity + Mean.Sea.Level.PressureIn + Mean.VisibilityMiles + Mean.Wind.SpeedMPH + new_precipitation + CloudCover + City + weekday + rushhour + weekend_holiday + feellike + season + BeautifulWeather, data = na.omit(linearreg_train))
summary(m_full_linear)
```
```{r}
BIC_model <- lm(total_rides ~ hour + Subscription.Type + Mean.TemperatureF + 
    Mean.Humidity + Mean.VisibilityMiles + Mean.Wind.SpeedMPH + 
    new_precipitation + City + feellike + season + BeautifulWeather, data = na.omit(linearreg_train))
summary(BIC_model)
```
```{r}
AIC_model <- lm(total_rides ~ hour + Subscription.Type + Mean.TemperatureF + 
    Mean.Humidity + Mean.VisibilityMiles + Mean.Wind.SpeedMPH + 
    new_precipitation + CloudCover + City + weekday + weekend_holiday +feellike + 
    season + BeautifulWeather, data = na.omit(linearreg_train))
summary(AIC_model)
```

We then used each model to make prediction on test dataset and analyzed performance based on MAE and MRSE.

Adjusted R Squared:  MAE of 60.25 and MRSE of 102.0942

BIC:  MAE of 60.28 and MRSE of 102.094

AIC:  MAE of 60.25 and MRSE of 102.1148

Additionally all three models resulted in an Adjusted R Squared of ~47%, which likely explains why all three models performed nearly the same.

```{r}
mfull_pred <- predict(m_full_linear,linearreg_test)
linearreg_test$total_rides_mfull_pred=mfull_pred
```
```{r}
BIC_pred <- predict(BIC_model,linearreg_test)
linearreg_test$total_rides_BIC_pred=BIC_pred
```
```{r}
AIC_pred <- predict(AIC_model,linearreg_test)
linearreg_test$total_rides_AIC_pred=AIC_pred
```
```{r}
MAE <- function(actual,predicted){
        mean(abs(actual - predicted), na.rm = TRUE)
}
```
```{r}
MAE(linearreg_test$total_rides,linearreg_test$total_rides_mfull_pred)
MAE(linearreg_test$total_rides,linearreg_test$total_rides_BIC_pred)
MAE(linearreg_test$total_rides,linearreg_test$total_rides_AIC_pred) 
```
```{r}
rmse(linearreg_test$total_rides,linearreg_test$total_rides_mfull_pred)
rmse(linearreg_test$total_rides,linearreg_test$total_rides_AIC_pred)
rmse(linearreg_test$total_rides,linearreg_test$total_rides_BIC_pred)
```

\section{Model 2 - Regression Tree Model}

Regression Tree Model (using similar variables of AIC model)
```{r}
rpart_model <- rpart(total_rides ~ hour + Subscription.Type + Mean.TemperatureF + 
    Mean.Humidity + Mean.VisibilityMiles + Mean.Wind.SpeedMPH + 
    new_precipitation + CloudCover + City + weekday + weekend_holiday +feellike + 
    season + BeautifulWeather, na.omit(linearreg_train))
```
```{r}
#Plot the tree
rpart.plot(rpart_model,digits = 3,fallen.leaves = TRUE,type = 4) # Plot the tree
fancyRpartPlot(rpart_model)
```

Predict on test dataset for regression tree model

```{r}
rpart_pred<-predict(rpart_model,linearreg_test)
linearreg_test$total_rides_rpart_pred=rpart_pred # Save the predictions as variable total_rides_pred_rpart on test
```

Measure model performance with Mean Absolute Error (MEA) to evaluate the model

```{r}
MAE(linearreg_test$total_rides,linearreg_test$total_rides_rpart_pred)
```

Measure model fit with Root Mean Square Error (RMSE) to evaluate the standard deviation of the model prediction error. A smaller value indicates better model performance.

```{r}
rmse(linearreg_test$total_rides,linearreg_test$total_rides_rpart_pred)
```

\section{Model 3 - Random Forest Model}

Random Forest Model (using similar variables of AIC model)
```{r}
set.seed(123)
rf_model <- randomForest(total_rides ~ hour + Subscription.Type + Mean.TemperatureF + 
    Mean.Humidity + Mean.VisibilityMiles + Mean.Wind.SpeedMPH + 
    new_precipitation + City + feellike + season + BeautifulWeather, data=linearreg_train,importance=TRUE,na.action=na.omit)
rf_model
```
```{r}
plot(rf_model, main="Random Forest") # Plot model accuracy by class
importance(rf_model) # Look variable importance
varImpPlot(rf_model, main="Random Forest by variable importance")
```

Predict on test dataset for Random Forest
```{r}
rf_pred<-predict(rf_model,linearreg_test)
linearreg_test$total_rides_rf_pred=rf_pred # Save the predictions as variable total_rides_pred_rf on test
```

Measure model performance with Mean Absolute Error (MEA) to evaluate the model
```{r}
MAE(linearreg_test$total_rides,linearreg_test$total_rides_rf_pred)

```

Measure model fit with Root Mean Square Error (RMSE) to evaluate the standard deviation of the model prediction error. A smaller value indicates better model performance.

```{r}
rmse(linearreg_test$total_rides,linearreg_test$total_rides_rf_pred)
```

Save all the predictions by day and hour
```{r}
linearreg_test[is.na(linearreg_test)]<-0

predictions_df<-as.data.frame(linearreg_test) %>%
  group_by(date,hour)%>%
  summarise(real=sum(total_rides),
            predictions_mfull=sum(total_rides_mfull_pred),
            predictions_bic=sum(total_rides_BIC_pred),
            predictions_aic=sum(total_rides_AIC_pred),
            predictions_rpart=sum(total_rides_rpart_pred),
            predictions_rf=sum(total_rides_rf_pred))
head(predictions_df)
write.csv(predictions_df,file="predictions_df.csv",row.names=FALSE)
```

\section{4. Discussion}
Comparing the MAE and MRSE across the models that showcased that the Random Forest model provided the most accurate predictions of hourly ridership.


\section{Appendix}
\section{I. Authors' Individual Contribution}
Elvin did the data exploration, Hellen designed the Random Forest Model, Lee designed the Linear Models, and Tarek design the Regression Tree model. All the team members contributted to the data collection, preprocessing, and analysis.

\section{References}
R Core Team (2016). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. Retrieved from http://www.R-project.org/.
References
http://wtop.com/sprawl-crawl/2016/03/study-ranks-d-c-traffic-2nd-worst-u-s/
https://www.thrillist.com/health/washington-dc/bike-lanes-dc-traffic-health-social-life
http://www.vice.com/read/reasons-why-washington-dc-is-the-worst-place-ever
https://www.washingtonpost.com/local/trafficandcommuting/as-metro-struggles-capital-bikeshare-takes-bigger-role-in-regions-transit-network/2016/11/12/b667a91a-a685-11e6-8042-f4d111c862d1_story.html?utm_term=.8a53a21312b5
https://www.washingtonpost.com/local/trafficandcommuting/new-regulations-for-uber-and-lyft-open-the-door-for-expansion/2015/02/21/8445149a-b83e-11e4-a200-c008a01a6692_story.html?utm_term=.f858bd2a92fb
http://dc.curbed.com/2015/9/14/9921646/capital-
http://www.wheretraveler.com/washington-dcbikeshare-data
https://urbanscrawldc.com/2015/10/06/how-the-dc-commute-really-stacks-up/
https://people.duke.edu/~rnau/compare.htm
http://statweb.stanford.edu/~jtaylo/courses/stats203/notes/selection.pdf
http://blog.minitab.com/blog/adventures-in-statistics/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit
http://www.statmethods.net/advstats/cart.html
https://www.r-bloggers.com/a-brief-tour-of-the-trees-and-forests/
http://www.eumetcal.org/resources/ukmeteocal/verification/www/english/msg/ver_cont_var/uos3/uos3_ko1.htm

\section {Summary of Analysis}
In summary, we selected the topic of Washington DC Capital Bikeshare ridership to be the focus of our analysis.  We felt this is a relevant and impactful topic for research given Washington DC's traffic issue and the increase in alternative transportation methods as a result (the metro, rideshare, bikeshare, etc).  Specific to the bikeshare program, we felt that it was an important task to be able to accurately predict the demand for the bikeshare program in order to maximize success and impact of such a program.

Therefore, we set out to accomplish two main tasks with this analysis:

1.  Identify which variables impact hourly ridership the most.
2.  Create a model to predict hourly bikeshare demand in the Greater Washington DC region.

The exploratory analysis suggested that the variables of City, Ridership Type, Feels Like Temperature, Hour of Day, and Day of Week would be the most impactful variables within our models.

We tested three modeling techniques and compared the model results using Mean Absolute Error and Root Mean Squared Error as the measure of success / accuracy.  The model techniques that we tested were the Linear Model (using adjusted R squared, BIC, and AIC variable selection techniques), the Regression Tree, and the Random Forrest.

Across the three modeling techniques it turned out that the models were in agreement that City, Hour of Day, and Membership Type were the most impactful variables in the prediction models.  

We concluded that the Random Forrest model provided the most accurate predictions, providing a Mean Absolute Error of 18.66 and a Root Mean Squared Error of 50.77.  This translates to meaning that for every hourly prediction that the model makes we can expect a potential error of +/- 18.66 on average.  

For future research, we believe that the prediction model could be improved.  We hit the limits of our available computing power, which limited the volume of data, the number of variables, and the models that we could create.  In future analysis, we recommend using a larger set of available data spanning multiple years, conducting additional feature engineering, and testing additional types of predictions models.
